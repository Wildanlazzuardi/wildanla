{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di Halaman Tugas Penambangan Data \u00b6 Nama : Wildan Lazzuardi Nim : 180411100037 kelas : Pendat 5D Alamat : Jl. Kusuma Bangsa No. 14 \u00b6","title":"Index"},{"location":"#selamat-datang-di-halaman-tugas-penambangan-data","text":"Nama : Wildan Lazzuardi Nim : 180411100037 kelas : Pendat 5D Alamat : Jl. Kusuma Bangsa No. 14","title":"Selamat Datang di Halaman Tugas Penambangan Data"},{"location":"Cara Menggunakan Metode Fuzzy C-Means/","text":"\u200b Cara Menggunakan Metode Fuzzy C-Means \u200b Fuzzy clustering (juga disebut sebagai soft clustering atau soft k-means) adalah bentuk pengelompokan di mana setiap titik data dapat menjadi milik lebih dari satu cluster. \u200b Clustering atau analisis cluster melibatkan penugasan poin data ke cluster sehingga item dalam cluster yang sama adalah sama mungkin, sementara item milik cluster berbeda sama mungkin. Cluster diidentifikasi melalui langkah-langkah kesamaan. Langkah-langkah kesamaan ini mencakup jarak, konektivitas, dan intensitas. Langkah-langkah kesamaan yang berbeda dapat dipilih berdasarkan data atau aplikasi. Berikut cara menggunakan Metode Fuzzy C-Means Nama Rumah Motor Wildan 1 3 Imam 3 3 Liya 4 3 Ang 5 3 Nia 1 2 Nadia 4 2 Julius 1 1 Isol 2 1 Jumlah Cluster 3 Max Iterasi 100 Pembobot 2 Epsilon 0,000001 Keanggotaan Cluster (random) C1 C2 C3 Jumlah Wildan 0,3 0,3 0,4 1 Imam 0,3 0,5 0,2 1 Liya 0,8 0,1 0,1 1 Ang 0,5 0,2 0,3 1 Nia 0,5 0,1 0,4 1 Nadia 0,2 0,1 0,7 1 Julius 0,3 0,4 0,3 1 Isol 0,6 0,2 0,2 1 Proses diatas adalah iterasi ke 1 Keanggotaan 0,3 0,3 0,4 0,3 0,5 0,2 0,8 0,1 0,1 0,5 0,2 0,3 0,5 0,1 0,4 0,2 0,1 0,7 0,3 0,4 0,3 0,6 0,2 0,2 Miu Kuadrat 0,09 0,09 0,16 0,09 0,25 0,04 0,64 0,01 0,01 0,25 0,04 0,09 0,25 0,01 0,16 0,04 0,01 0,49 0,09 0,16 0,09 0,36 0,04 0,04 Miu Kuadrat X1 0,09 0,27 0,27 0,27 2,56 1,92 1,25 0,75 0,25 0,5 0,16 0,08 0,09 0,09 0,72 0,36 Miu Kuadrat X2 0,09 0,27 0,75 0,75 0,04 0,03 0,2 0,12 0,01 0,02 0,04 0,02 0,16 0,16 0,08 0,04 Miu Kuadrat X3 0,16 0,48 0,12 0,12 0,04 0,03 0,45 0,27 0,16 0,32 1,96 0,98 0,09 0,09 0,08 0,04 Total Miu Kuadrat 1,81 0,61 1,08 Total Miu Kuadrat X 5,39 4,24 1,37 1,41 3,06 2,33 Pusat Cluster 2,977900552 2,342541436 2,245901639 2,31147541 2,833333333 2,157407407 X_V 4,344342358 2,02634 4,071073 0,432740148 1,04273 0,73774 1,476939043 3,55093 2,071073 4,521137938 8,05912 5,404407 4,029425231 1,64929 3,385888 1,162021916 3,17388 1,385888 5,714508104 3,27224 4,700703 2,758706999 1,78044 2,034036 L 0,390991 0,18237 0,651372 0,038947 0,260683 0,02951 0,945241 0,035509 0,020711 1,130284 0,322365 0,486397 1,007356 0,016493 0,541742 0,046481 0,031739 0,679085 0,514306 0,523558 0,423063 0,993135 0,071217 0,081361 Total L 1,224733 0,329139 1,001461 1,939046 1,565591 0,757305 1,460927 1,145713 LT 0,230184 0,493501 0,245635 2,310856 0,959021 1,355491 0,677076 0,281617 0,482841 0,221183 0,124083 0,185034 0,248174 0,606322 0,295343 0,860569 0,315072 0,721559 0,174993 0,305601 0,212734 0,362489 0,56166 0,491633 Total LT 0,969321 4,625367 1,441534 0,5303 1,14984 1,8972 0,693329 1,415782 Fungsi Objective 9,423915498 Selisih Fungsi Objective 9,423915498 Hasil belum ditemukan karena fungsi objective masih lebih besar dari episilon,maka iterasi dilanjutkan Iterasi ke-2 Keanggotaan 0,237469717 0,509120519 0,253409764 0,499604788 0,207339347 0,293055865 0,469691319 0,195358962 0,334949718 0,417090485 0,23398618 0,348923335 0,215833788 0,52731009 0,256856122 0,45359953 0,166072104 0,380328366 0,25239578 0,440774003 0,306830217 0,256034173 0,396713791 0,347252036 Miu Kuadrat 0,05639 0,259204 0,064217 0,2496 0,04299 0,085882 0,22061 0,038165 0,112191 0,17396 0,05475 0,121747 0,04658 0,278056 0,065975 0,20575 0,02758 0,14465 0,0637 0,194282 0,094145 0,06555 0,157382 0,120584 Miu Kuadrat X1 0,056392 0,169176 0,748815 0,748815 0,88244 0,66183 0,869822 0,521893 0,046584 0,093168 0,82301 0,411505 0,063704 0,063704 0,131107 0,065553 Miu Kuadrat X2 0,259204 0,777611 0,128969 0,128969 0,15266 0,114495 0,273748 0,164249 0,278056 0,556112 0,11032 0,05516 0,194282 0,194282 0,314764 0,157382 Miu Kuadrat X3 0,064217 0,19265 0,257645 0,257645 0,448765 0,336574 0,608737 0,365242 0,065975 0,13195 0,578599 0,289299 0,094145 0,094145 0,241168 0,120584 Total Miu Kuadrat 1,08217 1,052407 0,809391 Total Miu Kuadrat X 3,621874 2,735644 1,712002 2,148259 2,359251 1,788089 Pusat Cluster 3,34687727 2,527936162 1,626748139 2,041280982 2,914848615 2,209179979 Fungsi Objective 6,700351955 Selisih Fungsi Objective 2,723563543 X_V 5,730677189 1,31196 4,292042 0,343168108 2,80496 0,632647 0,649413567 6,55147 1,80295 2,955659027 12,298 4,973253 5,786549513 0,39452 3,710401 0,705285891 5,63403 1,22131 7,842421837 1,47708 5,128761 4,148667296 1,22358 2,299064 L 0,323164 0,340064 0,27562 0,085656 0,120584 0,054333 0,143267 0,250038 0,202275 0,51418 0,673308 0,605481 0,269562 0,109698 0,244794 0,145114 0,155386 0,176662 0,499591 0,28697 0,482846 0,27196 0,19257 0,27723 Total L 0,938847 0,260574 0,59558 1,792969 0,624054 0,477163 1,269406 0,74176 LT 0,174499 0,762221 0,232989 2,914024 0,356511 1,58066 1,539851 0,152638 0,554647 0,338334 0,081314 0,201076 0,172815 2,534743 0,269513 1,417865 0,177493 0,818793 0,127512 0,677012 0,194979 0,241041 0,817272 0,43496 Total LT 1,16971 4,851195 2,247135 0,620724 2,97707 2,414151 0,999502 1,493273 Hasil belum ditemukan karena fungsi objective masih lebih besar dari episilon,maka iterasi dilanjutkan. iterasi ke-3 Keanggotaan 0,149181823 0,651632577 0,199185601 0,60068165 0,073489311 0,325829039 0,685250691 0,067925417 0,246823892 0,545063625 0,130999034 0,323937341 0,058048538 0,851421975 0,090529487 0,587314097 0,073521876 0,339164027 0,127575139 0,677348903 0,195075958 0,161418112 0,54730249 0,291279399 Miu Kuadrat 0,02226 0,424625 0,039675 0,36082 0,005401 0,106165 0,46957 0,004614 0,060922 0,29709 0,017161 0,104935 0,00337 0,724919 0,008196 0,34494 0,005405 0,115032 0,01628 0,458802 0,038055 0,02606 0,29954 0,084844 Miu Kuadrat X1 0,022255 0,066766 1,082455 1,082455 1,878274 1,408706 1,485472 0,891283 0,00337 0,006739 1,379751 0,689876 0,016275 0,016275 0,052112 0,026056 Miu Kuadrat X2 0,424625 1,273875 0,016202 0,016202 0,018455 0,013842 0,085804 0,051482 0,724919 1,449839 0,021622 0,010811 0,458802 0,458802 0,59908 0,29954 Miu Kuadrat X3 0,039675 0,119025 0,318494 0,318494 0,243688 0,182766 0,524677 0,314806 0,008196 0,016391 0,460129 0,230064 0,038055 0,038055 0,169687 0,084844 Total Miu Kuadrat 1,54038 1,940467 0,557823 Pusat Cluster 3,843196325 2,718919184 1,210795859 1,84202705 3,23149123 2,338456049 Fungsi Objective 4,623720782 Selisih Fungsi Objective 2,076631173 X_V 8,162771765 1,38534 5,417194 0,789986467 4,54215 0,491229 0,103593818 9,12056 1,028246 1,417201169 15,699 3,565264 8,600610133 0,06939 5,094106 0,541432185 7,80462 0,705158 11,0384485 0,75344 6,771018 6,352055851 1,33185 3,308035 L 0,181664 0,588248 0,214927 0,285042 0,024531 0,052151 0,048644 0,042081 0,062643 0,421042 0,269406 0,374122 0,028981 0,050302 0,041749 0,18676 0,042188 0,081116 0,179655 0,345681 0,257669 0,165508 0,398943 0,280666 Total L 0,984839 0,361723 0,153368 1,064571 0,121032 0,310064 0,783005 0,845117 LT 0,122507 0,721846 0,184597 1,265844 0,22016 2,035712 9,653086 0,109642 0,97253 0,705616 0,063698 0,280484 0,116271 14,41123 0,196305 1,846953 0,128129 1,418121 0,090592 1,327238 0,147688 0,157429 0,750834 0,302294 Total LT 1,028951 3,521717 10,73526 1,049799 14,7238 3,393204 1,565519 1,210557 Hasil belum ditemukan karena fungsi objective masih lebih besar dari episilon,maka iterasi dilanjutkan. setelah iterasi ke20 baru ditemukan, selisih fungsi objective lebih kecil dari episilon. Hasil Clustering Keanggotaan Cluster Wildan C2 Imam C3 Liya C1 Ang C1 Nia C2 Nadia C1 Julius C2 Isol C2","title":"Fuzzy C-Means"},{"location":"Decision Tree/","text":"Decision Tree \u00b6 Decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan. Cara Membuat Decision Tree \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Decision Tree"},{"location":"Decision Tree/#decision-tree","text":"Decision tree merupakan metode klarifikasi yang sering digunakan atau metode paling polpuler ,keunggulannya adalah mudah di interprestasi oleh manusia .dicision tree merupakan suatu prediksi yang berupa pohon atau bisa disebut stuktur beriharki,konsep decision tree adalah mengubah data yang ada menjadi pohon keputusan dan aturan aturan keputusan.","title":"Decision Tree"},{"location":"Decision Tree/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]}});","title":"Cara Membuat Decision Tree"},{"location":"Mengukur Jarak/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski Distance.Minkowski Distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Uuclidean Distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Average Distance $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b \u200b","title":"***Mengukur Jarak Data***"},{"location":"Mengukur Jarak/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak/#mengukur-jarak-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak Numerik"},{"location":"Mengukur Jarak/#minkowski-distance","text":"kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski Distance.Minkowski Distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$","title":"Minkowski Distance"},{"location":"Mengukur Jarak/#uuclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini","title":"Uuclidean Distance"},{"location":"Mengukur Jarak/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Average Distance $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Average Distance"},{"location":"Mengukur Jarak/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b \u200b","title":"Manhattan distance"},{"location":"clustering/","text":"Clustering Categorical Data With k-Modes, K-Means, K-Prototype \u00b6 INTRODUCTION Analisis Clustering adalah proses yang tidak terawasi untuk mempartisi sekelompok objek data menjadi cluster, dengan tujuan untuk mengelompokkan objek yang memiliki kesamaan tinggi ke dalam cluster yang sama, sambil memisahkan benda yang berbeda menjadi kelompok yang berbeda. Clustering adalah tugas utama dari analisis data, dan itu sudah dipelajari secara luas di bidang penambangan data dan pembelajaran mesin. Teknik pengelompokan dapat secara kasar dibedakan sebagai pengelompokan keras dan lunak, penelitian ini terbatas pada pengelompokan keras analisis, di mana setiap objek milik satu dan hanya satu kluster. Dalam penelitian penambangan data, banyak upaya telah dilakukan pada pengembangan teknik baru untuk pengelompokan data kategorikal Prosedur Pengelompokan \u00b6 Untuk menentukan clustering itu tidak sembarangan, tetapi ada prosedur pengelompokannya yang terdiri dari empat langkah dengan jalur umpan balik. berikut adalah prosedur dalam bentuk flowchart. Dari gambar diatas diketahui bahwa prosedur pengelompokan ada 4 tahapan, yaitu : Pertama adalah Pre-processing yaitu ada data yang akan di kelompokan menjadi cluster, kemudian proses pemilihan fitur dan normalisasi Kemudian adalah Clustering yaitu pemilihan algoritma dan fungsi jarak pemilihan parameter algoritma penerapan algoritma Setelah itu didapat kandidat partisi yang mana digunakan untuk pemilihan teknik validasi dan penerapan teknik validasi Dan proses akhir adalah didapat kelompok partisi yang sebenarnya yang tidak dapat dikelompokkan lagi, lalu didapatlah informasi. Fungsi Jarak \u00b6 \u200b Untuk menghitung jarak (atau ketidaksamaan) antara dua objek X dan Y yang dijelaskan oleh atribut k kategori, fungsi jarak dalam mode-k didefinisikan sebagai: Proses Clustering \u00b6 \u200b Untuk mengelompokkan set data kategorikal X ke dalam k cluster, proses pengelompokan k-mode terdiri dari langkah-langkah berikut: Langkah 1: Pilih secara acak k objek unik sebagai pusat cluster awal (mode). Langkah 2: Hitung jarak antara setiap objek dan mode kluster; menetapkan objek ke cluster yang pusatnya memiliki jarak terpendek ke objek; ulangi langkah ini sampai semua objek ditugaskan ke cluster. Langkah 3: Pilih mode baru untuk setiap kluster dan bandingkan dengan mode sebelumnya. Jika berbeda, kembali ke Langkah 2; jika tidak, berhentilah. Pengetian Rumus K-Means \u00b6 \u200b K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas. Metode K-Modes \u00b6 K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Rumus K-Modes \u00b6 Metode K-Prototype \u00b6 Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Rumus K-Prototype \u00b6","title":"Clustering"},{"location":"clustering/#clustering-categorical-data-with-k-modes-k-means-k-prototype","text":"INTRODUCTION Analisis Clustering adalah proses yang tidak terawasi untuk mempartisi sekelompok objek data menjadi cluster, dengan tujuan untuk mengelompokkan objek yang memiliki kesamaan tinggi ke dalam cluster yang sama, sambil memisahkan benda yang berbeda menjadi kelompok yang berbeda. Clustering adalah tugas utama dari analisis data, dan itu sudah dipelajari secara luas di bidang penambangan data dan pembelajaran mesin. Teknik pengelompokan dapat secara kasar dibedakan sebagai pengelompokan keras dan lunak, penelitian ini terbatas pada pengelompokan keras analisis, di mana setiap objek milik satu dan hanya satu kluster. Dalam penelitian penambangan data, banyak upaya telah dilakukan pada pengembangan teknik baru untuk pengelompokan data kategorikal","title":"Clustering Categorical Data With k-Modes, K-Means, K-Prototype"},{"location":"clustering/#prosedur-pengelompokan","text":"Untuk menentukan clustering itu tidak sembarangan, tetapi ada prosedur pengelompokannya yang terdiri dari empat langkah dengan jalur umpan balik. berikut adalah prosedur dalam bentuk flowchart. Dari gambar diatas diketahui bahwa prosedur pengelompokan ada 4 tahapan, yaitu : Pertama adalah Pre-processing yaitu ada data yang akan di kelompokan menjadi cluster, kemudian proses pemilihan fitur dan normalisasi Kemudian adalah Clustering yaitu pemilihan algoritma dan fungsi jarak pemilihan parameter algoritma penerapan algoritma Setelah itu didapat kandidat partisi yang mana digunakan untuk pemilihan teknik validasi dan penerapan teknik validasi Dan proses akhir adalah didapat kelompok partisi yang sebenarnya yang tidak dapat dikelompokkan lagi, lalu didapatlah informasi.","title":"Prosedur Pengelompokan"},{"location":"clustering/#fungsi-jarak","text":"\u200b Untuk menghitung jarak (atau ketidaksamaan) antara dua objek X dan Y yang dijelaskan oleh atribut k kategori, fungsi jarak dalam mode-k didefinisikan sebagai:","title":"Fungsi Jarak"},{"location":"clustering/#proses-clustering","text":"\u200b Untuk mengelompokkan set data kategorikal X ke dalam k cluster, proses pengelompokan k-mode terdiri dari langkah-langkah berikut: Langkah 1: Pilih secara acak k objek unik sebagai pusat cluster awal (mode). Langkah 2: Hitung jarak antara setiap objek dan mode kluster; menetapkan objek ke cluster yang pusatnya memiliki jarak terpendek ke objek; ulangi langkah ini sampai semua objek ditugaskan ke cluster. Langkah 3: Pilih mode baru untuk setiap kluster dan bandingkan dengan mode sebelumnya. Jika berbeda, kembali ke Langkah 2; jika tidak, berhentilah.","title":"Proses Clustering"},{"location":"clustering/#pengetian-rumus-k-means","text":"\u200b K-Means adalah salah satu algoritma clustering / pengelompokan data yang bersifat Unsupervised Learning, yang berarti masukan dari algoritma ini menerima data tanpa label kelas.","title":"Pengetian Rumus K-Means"},{"location":"clustering/#metode-k-modes","text":"K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"clustering/#rumus-k-modes","text":"","title":"Rumus K-Modes"},{"location":"clustering/#metode-k-prototype","text":"Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"clustering/#rumus-k-prototype","text":"","title":"Rumus K-Prototype"},{"location":"fuzzy clustering/","text":"fuzzy clustering \u00b6 pengertian fuzzy clustering: \u00b6 Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"Program Fuzzy C-Means"},{"location":"fuzzy clustering/#fuzzy-clustering","text":"","title":"fuzzy clustering"},{"location":"fuzzy clustering/#pengertian-fuzzy-clustering","text":"Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"pengertian fuzzy clustering:"},{"location":"jarak/","text":"Menghitung Jarak Data \u00b6 Jarak Minkowski \u00b6 Jarak Minkowski adalah metrik dalam ruang vektor numerik yang dapat dianggap sebagai generalisasi dari jarak Euclidean dan jarak Manhattan. Jarak Minkowski dapat digambarkan dengan : $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Keterangan : m = bilangan riel positif xi dan yi = dua vektor dalam n Jarak Manhattan \u00b6 Jarak Manhattan adalah jarak antara dua titik diukur sepanjang sumbu di sudut kanan. Pada jarak manhattan sama seperti jarak minkowski, yaitu jarak manhattan sensitif terhadap outlier dengan m = 1. Jarak manhattan digambarkan dengan : $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ Jarak Euclidean \u00b6 Jarak Euclidean antara dua titik di bidang atau ruang 3 dimensi mengukur panjang segmen yang menghubungkan dua titik. Teorema Pythagoras dapat digunakan untuk menghitung jarak antara dua titik, seperti pada rumus yang berikut ini : $$ \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$ Jarak Average \u00b6 Jarak Average adalah jarak yang menggunakan rata-rata dari jarak euclidean untuk memperbaiki hasil. Untuk dua titik yaitu x dan y dalam dimensi n. Jarak Average dapat digambarkan pada rumus : $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Jarak Weighted Euclidean \u00b6 Jarak Weighted Euclidean adalah pengukuran jarak dengan menggunakan modifikasi dari jarak Euclidean. Perhitungannya berdasarkan tingkatan penting dari masing-masing atribut yang ditentukan. $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Keterangan : wi = bobot pada atribut ke i Jarak Chord \u00b6 Jarak Chord adalah cara perhitungan melalui modifikasi jarak Euclidean untuk mengatasi kekurangan dari jarak tersebut. Data yang tidak dinormalisasi juga dapat digunakan untuk Jarak Chord. Jarak Chord dapat dirumuskan sebagai berikut : $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ Jarak Mahalanobis \u00b6 ekstraksi hyperellipsoidal clusters didapat dari Jarak Mahalanobis yang teratur. Distorsi yang disebabkan oleh korelasi linear antara fitur dapat diatasi dengan jarak Mahalanobis. Rumus jarak Mahalanobis dapat digambarkan dengan : $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Keterangan : S = matrik covariance data","title":"Mengukur Jarak"},{"location":"jarak/#menghitung-jarak-data","text":"","title":"Menghitung Jarak Data"},{"location":"jarak/#jarak-minkowski","text":"Jarak Minkowski adalah metrik dalam ruang vektor numerik yang dapat dianggap sebagai generalisasi dari jarak Euclidean dan jarak Manhattan. Jarak Minkowski dapat digambarkan dengan : $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Keterangan : m = bilangan riel positif xi dan yi = dua vektor dalam n","title":"Jarak Minkowski"},{"location":"jarak/#jarak-manhattan","text":"Jarak Manhattan adalah jarak antara dua titik diukur sepanjang sumbu di sudut kanan. Pada jarak manhattan sama seperti jarak minkowski, yaitu jarak manhattan sensitif terhadap outlier dengan m = 1. Jarak manhattan digambarkan dengan : $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Jarak Manhattan"},{"location":"jarak/#jarak-euclidean","text":"Jarak Euclidean antara dua titik di bidang atau ruang 3 dimensi mengukur panjang segmen yang menghubungkan dua titik. Teorema Pythagoras dapat digunakan untuk menghitung jarak antara dua titik, seperti pada rumus yang berikut ini : $$ \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$","title":"Jarak Euclidean"},{"location":"jarak/#jarak-average","text":"Jarak Average adalah jarak yang menggunakan rata-rata dari jarak euclidean untuk memperbaiki hasil. Untuk dua titik yaitu x dan y dalam dimensi n. Jarak Average dapat digambarkan pada rumus : $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Jarak Average"},{"location":"jarak/#jarak-weighted-euclidean","text":"Jarak Weighted Euclidean adalah pengukuran jarak dengan menggunakan modifikasi dari jarak Euclidean. Perhitungannya berdasarkan tingkatan penting dari masing-masing atribut yang ditentukan. $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ Keterangan : wi = bobot pada atribut ke i","title":"Jarak Weighted Euclidean"},{"location":"jarak/#jarak-chord","text":"Jarak Chord adalah cara perhitungan melalui modifikasi jarak Euclidean untuk mengatasi kekurangan dari jarak tersebut. Data yang tidak dinormalisasi juga dapat digunakan untuk Jarak Chord. Jarak Chord dapat dirumuskan sebagai berikut : $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Jarak Chord"},{"location":"jarak/#jarak-mahalanobis","text":"ekstraksi hyperellipsoidal clusters didapat dari Jarak Mahalanobis yang teratur. Distorsi yang disebabkan oleh korelasi linear antara fitur dapat diatasi dengan jarak Mahalanobis. Rumus jarak Mahalanobis dapat digambarkan dengan : $$ d _ { m a h } = \\sqrt { ( x - y ) S ^ { - 1 } ( x - y ) ^ { T } } $$ Keterangan : S = matrik covariance data","title":"Jarak Mahalanobis"},{"location":"missing value/","text":"Memperlakukan Missing Value Dengan Metode Algoritma K-Nearest Neighbor (KNN) \u00b6 Missing Value \u00b6 Missing value adalah informasi yang tidak tersedia untuk sebuah objek (kasus). Missing value terjadi karena informasi untuk sesuatu tentang objek tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada. Missing value pada dasarnya tidak bermasalah bagi keseluruhan data, apalagi jika jumlahnya hanya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang meng kita dapat menggunakan metode Algoritma K-Nearest Neighbor (KKN). K-Nearest Neighbor (KNN) \u00b6 Salah satu usaha untuk memperlakukan missing data yaitu dengan menggunakan Algoritma K-Nearest Neighbor (KNN). Lalu apa yang dimaksud dengan KNN? Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN . Algroritma pada K-Nearest Neighbor (KNN) \u00b6 Langkah utama dalam metode KNN yaitu dengan menghitung nilai k. Nilai k yang dimaksud yaitu jarak tetangga terdekat antar dataset. Kemudian, hasil perhitungan nilai k tersebut menjadi nilai estimator yang digunakan untuk mengisi pada data yang hilang tersebut. Perhitungan untuk mencari nilai k yaitu tergantung dengan jenis data. Apabila data yang disajikan berupa data kontinu, maka menggunakan rata-rata dari tetangga terdekat. Dan apabila data yang disajikan berupa data kualitatif, maka diambil dari nilai yang sering keluar pada objek. Dapat dimisalkan bahwa D merupakan suatu objek yang memiliki kasus missing data. Dengan Dc merupakan subdata yang lengkap, sedangkan Dm merupakan sub data yang memiliki kerumpangan (mengandung atribut yang hilang). Maka, tahapan langkah algoritma pada KKN sebagai berikut : Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'Score Pertama' :[ 100 , 80 , np . nan , 65 ], 'Score Kedua' : [ 80 , 55 , 76 , np . nan ], 'Score Ketiga' :[ np . nan , 60 , 90 , 87 ], 'Score Keempat' :[ np . nan , 50 , 65 , 75 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) Score Pertama Score Kedua Score Ketiga Score Keempat 0 100.0 80.0 60.0 75.0 1 80.0 55.0 60.0 65.0 2 0.0 76.0 90.0 0.0 3 65.0 0.0 87.0 60.0 4 75.0 0.0 77.0 50.0 MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b","title":"Missing value"},{"location":"missing value/#memperlakukan-missing-value-dengan-metode-algoritma-k-nearest-neighbor-knn","text":"","title":"Memperlakukan Missing Value Dengan Metode  Algoritma K-Nearest Neighbor (KNN)"},{"location":"missing value/#missing-value","text":"Missing value adalah informasi yang tidak tersedia untuk sebuah objek (kasus). Missing value terjadi karena informasi untuk sesuatu tentang objek tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada. Missing value pada dasarnya tidak bermasalah bagi keseluruhan data, apalagi jika jumlahnya hanya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang meng kita dapat menggunakan metode Algoritma K-Nearest Neighbor (KKN).","title":"Missing Value"},{"location":"missing value/#k-nearest-neighbor-knn","text":"Salah satu usaha untuk memperlakukan missing data yaitu dengan menggunakan Algoritma K-Nearest Neighbor (KNN). Lalu apa yang dimaksud dengan KNN? Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN .","title":"K-Nearest Neighbor (KNN)"},{"location":"missing value/#algroritma-pada-k-nearest-neighbor-knn","text":"Langkah utama dalam metode KNN yaitu dengan menghitung nilai k. Nilai k yang dimaksud yaitu jarak tetangga terdekat antar dataset. Kemudian, hasil perhitungan nilai k tersebut menjadi nilai estimator yang digunakan untuk mengisi pada data yang hilang tersebut. Perhitungan untuk mencari nilai k yaitu tergantung dengan jenis data. Apabila data yang disajikan berupa data kontinu, maka menggunakan rata-rata dari tetangga terdekat. Dan apabila data yang disajikan berupa data kualitatif, maka diambil dari nilai yang sering keluar pada objek. Dapat dimisalkan bahwa D merupakan suatu objek yang memiliki kasus missing data. Dengan Dc merupakan subdata yang lengkap, sedangkan Dm merupakan sub data yang memiliki kerumpangan (mengandung atribut yang hilang). Maka, tahapan langkah algoritma pada KKN sebagai berikut : Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'Score Pertama' :[ 100 , 80 , np . nan , 65 ], 'Score Kedua' : [ 80 , 55 , 76 , np . nan ], 'Score Ketiga' :[ np . nan , 60 , 90 , 87 ], 'Score Keempat' :[ np . nan , 50 , 65 , 75 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) Score Pertama Score Kedua Score Ketiga Score Keempat 0 100.0 80.0 60.0 75.0 1 80.0 55.0 60.0 65.0 2 0.0 76.0 90.0 0.0 3 65.0 0.0 87.0 60.0 4 75.0 0.0 77.0 50.0 MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b","title":"Algroritma pada K-Nearest Neighbor (KNN)"},{"location":"statistik deskriptif/","text":"Statistik Deskriptif \u00b6 Pengertian \u00b6 Pengertian statistik deskriptif metode pengumpulan sebuah data data yang akan menghasilkan informasi yang berguna Tipe statistik deskriptif \u00b6 Mean(Rata-rata) Mean atau rata rata adalah sebuah nilai yang jumlah dari seluruah angka atau data dan di bagi banyak data .misal memiliki N data bisa di hitung dengan rumus sebagai berikut $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i={a_1+a_2+a_3+a_4+........+a_n \\over n} \\end{align} $$ keterangan: x=rata-rata a=nilai ke N n=banyak nilai atau data Median median merupakan nilai tengah dalam suatu data median disimbolkan Me .menghitung median mempunyai 2 metode yaitu ketika N atau jumlah data ganjil atau genap. berikut rumus median ; $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ keterangan: me =median atau nilai tengah n=banyak data Modus \u00b6 Modus adalah nilai yang sering muncul dalam himpunan data.brikut ini rumus mencari modus dalam himpunan data $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ ket; mo=nilai modus tb= tepi bawah b1=selisih frekuensi antara nilai mudus dengan elemen sebelumnya b2=selisih frekuensi antara nilai mudus dengan elemen sesudahnya p= panjang interval Varian \u00b6 Varian adalah penyebaran nilai dalam suatu data dari rata rata .berikut ini rumus dari varian dalam himpunan data $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan: x=rata rata Xi=rata rata dari semua titik data n= banyak dari anggota data \u200b Standart Deviasi \u00b6 Standar deviasi adalah ukuran kumpulan data relatif terhadap rata-rata atau akar kuadrat positif dari varian. $$ t {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Skewness \u00b6 adalah ketidaksimetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri atau ke kanan. Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Quartile \u00b6 Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Penerapan Pada Python \u00b6 Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini, library python yang digunakan adalah sebagai berikut: pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Pertama py import pandas as pd from scipy import stats Kedua py df = pd.read_csv('sample_data.csv', sep=';') Ketiga py data = {\"Stats\" : ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes', 'Quartile 1','Quartile 2', 'Quartile 3', 'Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(), 2),round(df[i].var(), 2), round(df[i].skew(), 2), df[i].quantile(0.25), df[i].quantile(0.5), df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]] import pandas as pd from scipy import stats df = pd . read_csv ( 'wildan.csv' , sep = ';' ) data = { \"stats\" :[ 'Min' , 'Max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data ) tes . style . hide_index () stats ukuran baju ukuran sepatu ukaran celana umur Min 18 27 22 20 Max 30 42 36 40 Mean 23.76 34.452 28.812 29.828 Standart Deviasi 3.75 4.64 4.23 6.18 Variasi 14.05 21.53 17.93 38.18 Skewnes 0.08 0.02 0.08 -0 Quantile 1 21 30.75 25 24 Quantile 2 24 34 29 30 Quantile 3 27 39 32 35 Median 24 34 29 30 Modus 21 32 26 24 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik deskriptif"},{"location":"statistik deskriptif/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"statistik deskriptif/#pengertian","text":"Pengertian statistik deskriptif metode pengumpulan sebuah data data yang akan menghasilkan informasi yang berguna","title":"Pengertian"},{"location":"statistik deskriptif/#tipe-statistik-deskriptif","text":"Mean(Rata-rata) Mean atau rata rata adalah sebuah nilai yang jumlah dari seluruah angka atau data dan di bagi banyak data .misal memiliki N data bisa di hitung dengan rumus sebagai berikut $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i={a_1+a_2+a_3+a_4+........+a_n \\over n} \\end{align} $$ keterangan: x=rata-rata a=nilai ke N n=banyak nilai atau data Median median merupakan nilai tengah dalam suatu data median disimbolkan Me .menghitung median mempunyai 2 metode yaitu ketika N atau jumlah data ganjil atau genap. berikut rumus median ; $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ keterangan: me =median atau nilai tengah n=banyak data","title":"Tipe statistik deskriptif"},{"location":"statistik deskriptif/#modus","text":"Modus adalah nilai yang sering muncul dalam himpunan data.brikut ini rumus mencari modus dalam himpunan data $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ ket; mo=nilai modus tb= tepi bawah b1=selisih frekuensi antara nilai mudus dengan elemen sebelumnya b2=selisih frekuensi antara nilai mudus dengan elemen sesudahnya p= panjang interval","title":"Modus"},{"location":"statistik deskriptif/#varian","text":"Varian adalah penyebaran nilai dalam suatu data dari rata rata .berikut ini rumus dari varian dalam himpunan data $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan: x=rata rata Xi=rata rata dari semua titik data n= banyak dari anggota data \u200b","title":"Varian"},{"location":"statistik deskriptif/#standart-deviasi","text":"Standar deviasi adalah ukuran kumpulan data relatif terhadap rata-rata atau akar kuadrat positif dari varian. $$ t {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$","title":"Standart Deviasi"},{"location":"statistik deskriptif/#skewness","text":"adalah ketidaksimetrisan pada suatu distribusi statistik dimana kurva tampak condong ke kiri atau ke kanan. Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$","title":"Skewness"},{"location":"statistik deskriptif/#quartile","text":"Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$","title":"Quartile"},{"location":"statistik deskriptif/#penerapan-pada-python","text":"Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini, library python yang digunakan adalah sebagai berikut: pandas, digunakan untuk data manajemen dan data analysis. scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. Pertama py import pandas as pd from scipy import stats Kedua py df = pd.read_csv('sample_data.csv', sep=';') Ketiga py data = {\"Stats\" : ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes', 'Quartile 1','Quartile 2', 'Quartile 3', 'Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(), 2),round(df[i].var(), 2), round(df[i].skew(), 2), df[i].quantile(0.25), df[i].quantile(0.5), df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]] import pandas as pd from scipy import stats df = pd . read_csv ( 'wildan.csv' , sep = ';' ) data = { \"stats\" :[ 'Min' , 'Max' , 'Mean' , 'Standart Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data ) tes . style . hide_index () stats ukuran baju ukuran sepatu ukaran celana umur Min 18 27 22 20 Max 30 42 36 40 Mean 23.76 34.452 28.812 29.828 Standart Deviasi 3.75 4.64 4.23 6.18 Variasi 14.05 21.53 17.93 38.18 Skewnes 0.08 0.02 0.08 -0 Quantile 1 21 30.75 25 24 Quantile 2 24 34 29 30 Quantile 3 27 39 32 35 Median 24 34 29 30 Modus 21 32 26 24 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Penerapan Pada Python"}]}